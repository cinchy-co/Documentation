# 5.7 Release Notes

{% hint style="success" %}
Cinchy version 5.7 was released on XXXX
{% endhint %}

{% hint style="info" %}
For instructions on how to upgrade your platform to the latest version, please review the documentation [here.](../../deployment-guide/upgrade-guides/)
{% endhint %}

## New Capabilities

### **Connections**

**CIN-00687 UI: Test connection credentials without executing a sync**

We've made it simpler to debug invalid credentials in data syncs by adding a **"Test Connection" button** to the UI for the following sources and destinations:

| Name               | Source | Destination |
| ------------------ | ------ | ----------- |
| Amazon Marketplace | ✅      | 🚫          |
| Binary Files       | ✅      | N/A         |
| Copper             | ✅      | N/A         |
| DB2                | ✅      | ✅           |
| Delimited File     | ✅      | N/A         |
| Dynamics           | ✅      | 🚫          |
| Excel              | ✅      | N/A         |
| Fixed Width File   | ✅      | N/A         |
| Kafka Topic        | 🚫     | ✅           |
| ODBC               | ✅      | N/A         |
| Oracle             | ✅      | ✅           |
| Parquet            | ✅      | N/A         |
| Salesforce Object  | ✅      | ✅           |
| Snowflake          | ✅      | ✅           |
| MS SQL Server      | ✅      | ✅           |

Selecting this button will validate whether your username/password/connection string/etc. are able to connect to your source or destination. If successful, a "Connection Succeeded" popup will appear. If unsuccessful, a "Connection Failed" message will appear, along with the ability to review the associated troubleshooting logs. With this change, you are able to debug access-related data syncs at a more granular level.

<div data-full-width="true">

<figure><img src="../../.gitbook/assets/image (212).png" alt=""><figcaption></figcaption></figure>

</div>

<figure><img src="../../.gitbook/assets/image (223).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../.gitbook/assets/image (659).png" alt=""><figcaption></figcaption></figure>

**CIN-03878 Listener UI: Bring JSON fields to connections UI from Listener Config**&#x20;

As we continue to enhance our Connections Experience offerings, you can now configure your listener for real-time syncs directly in the UI without having to navigate to a separate table. For any event triggered sync source, **(CDC, REST API, Kafka Topic, MongoDB Event, Polling Event, Salesforce Platform Event, and Salesforce Push Topic)**, there is now the option to input your configurations directly from the Source tab in the Connections Experience. Any configuration you populate via the UI will be automatically reflected back into the Listener Config table of your platform.

You are able to set the:

* Topic JSON
* Connections Attributes
* Auto Offset Reset
* Listener Status (Enabled/Disabled)

<figure><img src="../../.gitbook/assets/image (237).png" alt=""><figcaption></figcaption></figure>

Information on the parameters and configurations for the above settings can be found [here](../../data-syncs/supported-real-time-sync-stream-sources/the-listener-configuration-table.md) and [here.](../../data-syncs/supported-real-time-sync-stream-sources/)

For ease of use, we have also added help tips to the UI, as well as examples where necessary. You can find information about&#x20;

{% hint style="info" %}
Note: If there is more than one listener associated with your data sync, you will need to configure it via the Listener Configuration table instead.
{% endhint %}

**CIN-03795 New Source: Oracle Polling Connector**

**We have added Oracle** as a new database type for Polling Events in Connections. Data Polling is a source option first featured in Cinchy v5.4 which uses the Cinchy Event Listener to continuously monitor and sync data entries from your Oracle, SQL Server, or DB2 server into your Cinchy table. This capability makes data polling a much easier, effective, and streamlined process and avoids implementing the complex orchestration logic that was previous necessary.

**CIN-04063 Connection Source Filters - Stage 2 - Conditional update UI**

**For REST API, SOAP 1.2, Kafka Topic, Platform Event, and Parquet sources**, we have added a new [**"Conditional" option for source filters**](../../data-syncs/building-data-syncs/advanced-settings/filters.md#1.-source-filters) in the Connections UI. Similarly to how the "Conditional Changed Record Behaviour" capability, works, once selected you will be able to define the conditions upon which data is pulled into your source via the filter. After data is pulled from the source, new conditional UI filters down the set of returned records to ones that match the defined conditions.&#x20;

<figure><img src="../../.gitbook/assets/image (657).png" alt=""><figcaption></figcaption></figure>

**CIN-04089 Secrets: Add API for Cinchy secret retrieval from Cinchy secrets data model**

The Cinchy platform now comes with a new way to store secrets — [the Cinchy Secrets Table. ](../../guides-for-using-cinchy/additional-guides/cinchy-secrets-manager.md)Adhering to Cinchy’s Universal Access Controls, you can utilize this table as a key vault (such as Azure Key Vault or AWS Secrets Manager) to store sensitive data only accesible to the users or user groups that you give access to.&#x20;

<figure><img src="../../.gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>

You can use secrets stored in this table [anywhere a regular variable can go](../../data-syncs/building-data-syncs/advanced-settings/variables.md) when configuring data syncs, including but not limited to:

* **As part of a connection string;**
* **Within a REST Header, URL, or Body;**
* **As an Access Key ID.**

You can also use it in a Listener Configuration.

Additionally, we have implemented a new [API endpoint](broken-reference) for the retrieval of your secrets. Using the below endpoint, fill in your \<base-url>, \<secret-name>, and the \<domain-name> to retrieve the referenced secret.

This endpoint works with Cinchy’s [Personal Access Token](../../guides-for-using-cinchy/user-guides/user-preferences/personal-access-tokens.md) capability, as well as Access Tokens retrieved from your IDP.

**Blank Example:**

```
<base-url>/api/v1.0/secrets-manager/secret?secretName=<secret-name>&domain=<domain-name>
```

**Populated Example:**&#x20;

```
Cinchy.net/api/v1.0/secrets-manager/secret?secretName=<ExampleSecret>&domain=<Sandbox>
```

The API will return an object in the below format:

```
{
    "secretValue": "password123"
}
```

\
**CIN-04249 Polling listener optimizations: productionize changes**

To improve your Connections experience, we have made various optimizations to our Polling Event Listener.

* We have added a new configurable property, **DataPollingConcurrencyIndex**, to the Data Polling Event Listener. This property allows only a certain number of threads to run queries against the source database, which works to reduce the load against the database. The default number of threads is set to 12. To configure this property, navigate to your **appSettings.json** deployment file **>  "DataPollingConcurrencyIndex": \<numberOfThreads>**
* We have added a new configurable property, **QueueWriteConcurrencyIndex**, to the Data Polling Event Listener. This property allows only a certain number of threads to be concurrently sending messages to the queue. This works to provide a more consistent batching by the worker and reduce your batching errors. run queries against the source database, which works to reduce the load against the database. The default number of threads is set to 12. To configure this property, navigate to your **appSettings.json** deployment file **>  "QueueWriteConcurrencyIndex": \<numberOfThreads>.** Note that this index is shared across all listener configs, meaning that if it is set to 1 only one listener config will be pushing the messages to the queue at a single moment in time.
* We have added a new mandatory property, **CursorConfiguration.CursorColumnDataType**,  to the Listener Topic for the Data Polling Event. This change was made in tandem with an update that ensure that the database query always moved the offset, regardless of if the query returned the records or not—this helps to ensure that the performance of the source database is not being weighed down by constantly running heavy queries on a wide range of records when the queries returned no data. This value of this mandatory property must match the column type of the source database system for proper casting of parameters.
* We have added a new configurable property, **CursorConfiguration.Distinct**, to the Listener Topic for the Data Polling Event. This property is a true/false Boolean type that, when set to true, applies a distinct clause on your query to avoid any duplicate records.

<pre class="language-json" data-overflow="wrap"><code class="lang-json"><strong>// App Settings JSON Example
</strong><strong>// Example of the new configurable propeties: DataPollingConcurrencyIndex (set to "1" and QueueWriteConcurrencyIndex (set to "1")
</strong>"AppSettings": {
    "GetNewListenerConfigsInterval": "",
    "StateFileWriteDelaySeconds": "",
    "KafkaClientConfig": {
      "BootstrapServers": ""
    },
    "KafkaRealtimeDatasyncTopic": "",
    "KafkaJobCancellationTopic": "",
    "DataPollingConcurrencyIndex":  1,
    "QueueWriteConcurrencyIndex":  1
  }
</code></pre>

{% code overflow="wrap" %}
```json
// Listener Config Topic Example
// Example of the new mandatory CursorColumnDataType property, which below is set to "int", and "Distinct", below set to "true".
{
   "CursorConfiguration": {
       "FromClause": "",
       "CursorColumn": "",
       "BatchSize": "",
       "FilterCondition": "",
       "Columns": [],
            "Distinct": "true"
            "CursorColumnDataType" : "int"
   },
        "Delay": ""
}
```
{% endcode %}

## Enhancements

### Connections

We have made various enhancements to the Connections Experience which should help to simplify and streamline your ability to create and maintain data synchronizations across the platform. Examples of these changes can be found in our [Data Sync documentation.](broken-reference)

* **UI changes,** including:
  * Converting drop-down menus to radio buttons for the following:
    * **Sync Strategy**
    * **Source Schema Data Types**
    * **Source Schema "Add Column"**
  * Increasing the width of the source, destination, and connections drop-down menus in order to prevent visibility losses on some screens
  * Improvement to the organization of file based source fields
  * Removing the following fields:
    * **Source > Cinchy Table > Model**
    * **Info > Version**
* **Order of Operations changes,** including:
  * Moving the **"Permissions"** step into the **"Info"** tab.
* **Verbiage changes,** including:
  * Changing the name of the **"Sync Behaviour"** tab to **"Sync Actions"**
  * Changing the term **"Parameters"** to **"Variables"**
  * Changing **"Sync Pattern"** to **"Sync Strategy"** in the Sync Behaviour (now Sync Actions) tab
  * Changing **"Column Mappings"** to **"Mappings"** in the Destination tab
  * Changing **"Access Token"** to **"API Key"** in the Copper Source in order to match the language presented in Copper's own documentation
  * Aligning file-based connector language across all sources
  * Adding descriptive text to various sections to help better guide you through the data sync configuration, including sections such as Mapping, Schema, Sync Behaviour, etc.
  * Various other verbiage changes throughout the UI

**CIN-03924 Job error log downloads: Can't download if error was not in records processing context**

To help simplify and streamline the Connections experience, you are now able to view the output for each job by clicking on the **Output** button located in the **Jobs** tab of the UI after you run a sync.&#x20;

<figure><img src="../../.gitbook/assets/Cinchy - Connections.png" alt=""><figcaption></figcaption></figure>

This links to the Execution Log table with a filter set for your specific sync, which can help you reach your execution related data quicker and easier than before.

<figure><img src="../../.gitbook/assets/execution log.jpg" alt=""><figcaption></figcaption></figure>

**CIN-04666 Log full REST Target HTTP response instead of just the status in execution errors**

We now log the full REST Target HTTP response in the data sync Execution Errors table to provide you with more detailed information about your job. This replaces the original log that only contained the HTTP response status code.

### Platform

**CIN-03837 Try to upgrade IdentityServer4 to IdentityServer6**

We have upgraded our IDP **from IdentityServer4 to IdentityServer6** to ensure we are maintaining the highest standard of security for your platform.

#### CIN-04699 Add Execute Function to UDF Extensions

We have added `execute`, a new method for UDF extensions. This new query call returns a `queryResult` object that contains additional information about your result. For more information, see the [Cinchy User Defined Functions page](../../cql/the-basics-of-cql/cinchy-supported-functions/cinchy-user-defined-functions-udfs/).







## Bugs

### Platform

**CIN-04421 Upgrade Utility should not run upgrade when -c command is specified**

A bug was fixed in the Cinchy Upgrade Utility that was causing **the use of the -c flag**, which is meant to delete extra metadata created on the database, to instead run (or rerun) the entire upgrade process.&#x20;

**CIN-04522 Query params on relative Application URLs in the applets table are stripped**

We have fixed a bug that was stripping query parameters from Relative URLs if they were being used as the Application URL for applets. In the below screenshot, the bug would have stripped out the "q=1" parameter, leaving only an Absolute URL in lieu of a Relative one.

<figure><img src="../../.gitbook/assets/image (175).png" alt=""><figcaption></figcaption></figure>

### Connections

**CIN-03929: Mongo Change Stream Source: If UUID/ObjectId is used, it is not converted into text**

We fixed a bug where the UUID/ObjectId in a MongoDB Change Stream Sourced data sync was not being serialized into text format. If you have any MongoDB Stream Sourced syncs currently utilizing the UUID/ObjectId, **you may need to adjust accordingly when referencing the columns with those data types.**

<pre class="language-json"><code class="lang-json">// Previous UUID/ObjectIDs would have been serialized as the below:
{
  "_id": ObjectId('644054f5f88104157fa9428e'),
  "uuid": UUID('ca8a3df8-b029-43ed-a691-634f7f0605f6')
}
<strong>
</strong>// They will now serialize into text format like this:
{
  "_id": "644054f5f88104157fa9428e",
  "uuid": "ca8a3df8-b029-43ed-a691-634f7f0605f6"
}
</code></pre>

**CIN-04182: Setting the user's time zone to UTC will cause tables not return any data due to broken queries**

We have fixed a bug where setting a user’s time zone to UTC (Coordinated Universal Time) would result in no data being returned in any tables.

**CIN-04277 Saving Saved Queries Nulls out Sync GUIDs (used in DXD)**

We have fixed a bug where the Sync GUID of Saved Queries transferred over via DXD would null out.

**CIN-04459 Mongo loses changestream cursor when auto offset reset = earliest**

We have fixed a bug affecting the MongoDB Event Listener wherein the “auto offset reset” functionality would not work as anticipated when set to “earliest".

**CIN-03924 Job error log downloads: Can't download if error was not in records processing context**

We have fixed a bug where failed jobs would return errors for logs that have not yet been created. Log files now correctly search for only the relevant logs for the failed job.\
\
C**IN-04302 - Event Listener Optimizations: Add queueing Semaphore in MongoDB Listener.**

We are continuing to provide optimization updates to our Connections capabilities. v5.7 of the Cinchy platform has the following updates for the MongoDB Event Stream:

* A new configurable property, ‘QueueWriteConcurrencyIndex’, to the MongoDB Event Listener. This property allows only a certain number of threads to be concurrently sending messages to the queue. This works to provide a more consistent batching by the worker and reduce your batching errors. run queries against the source database, which works to reduce the load against the database. The default number of threads is set to 12. To configure this property, navigate to the appSettings.json >  "QueueWriteConcurrencyIndex": \<numberOfThreads>. Note that this index is shared across all listener configs, meaning that if it is set to 1 - only one listener config will be pushing the messages to the queue at a single moment in time.
* We have also added a new optional property to the MongoDB Listener Topic, 'changeStreamSettings.batchsize’, that is a configurable way to set your own batch size on the MongoDB Change Stream Listener.

```json
// Example of the listener topic
{
   "database": "",
   "collection": "",
        "changeStreamSettings": {
       "pipelineStages": [],
       "batchSize": "1000"
   }
}
```

**CIN-05489 - Conditional calculated column for the Delimited File source is rendering the IF field incorrectly**

We have fixed an issue in the data configuration table where the `IF` field for the **Delimited File > Conditional Calculated Column** was not displaying correctly.

**CIN-05588 Downgrade commandline nuget package to resolve arg parsing for data syncs**

We have resolved an issue where using multiple parameters while configuring data syncs could result in parsing and execution errors.

**CIN-05429 Calculated columns should work for Mongo (target only?)**

We have fixed a bug preventing calculated columns from working in MongoDB targets for data syncs.

**CIN-04592 Only prompt users to ask if they want to restore a config in scenarios that make sense**

We have fixed a bug where users were prompted to restore unsaved changes for a new connection when no configuration changes to a data sync were made.

**CIN-04628 If System User is a part of the Connections (or any) group application fails to initialize**

We have fixed a bug that was causing the platform to fail upon initializing when a System User had been added to any user group (such as the Connections or Admin groups).
