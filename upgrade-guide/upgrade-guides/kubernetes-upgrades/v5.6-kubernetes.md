---
description: >-
  This page details the instructions for upgrading your Cinchy platform to v5.6
  on Kubernetes
---

# v5.6 (Kubernetes)

## Upgrading on Kubernetes

When it comes time to upgrade your various components, you can do so by following the below instructions.

{% hint style="info" %}
If you have made custom changes to your deployment file structure, please contact your Support team prior to upgrading your environments.
{% endhint %}

{% hint style="danger" %}
**Warning:** If you are upgrading from Cinchy v5.1 or lower to Cinchy v5.6, you must first run a mandatory process (Upgrade 5.2)\*\* [**using the Cinchy Utility**](../../upgrade-guides/cinchy-upgrade-utility.md) **and deploy version 5.2.**
{% endhint %}

{% hint style="danger" %}
**If you are upgrading from Cinchy v5.3 or lower to Cinchy v5.6 on an SQL Server Database**, you will need to make a change to your `connectionString`. Adding [**TrustServerCertificate=True**](https://learn.microsoft.com/en-us/dotnet/api/system.data.sqlclient.sqlconnectionstringbuilder.trustservercertificate?view=dotnet-plat-ext-6.0) will allow you to bypass the certificate chain during validation.

For a Kubernetes deployment, you can add this value in your **deployment.json file:**

```json
"cinchy_instance_configs": {
          "database_connection_string": "User ID=cinchy;Password=<password>;Host=<db_hostname>;Port=5432;Database=development;Timeout=300;Keepalive=300;TrustServerCertificate=True"}
```
{% endhint %}

{% hint style="danger" %}
**Warning:** If you are upgrading from Cinchy v5.4 or lower to Cinchy v5.6, you must first run a mandatory process (Upgrade 5.5) using\*\* [**the Cinchy Utility**](../../upgrade-guides/cinchy-upgrade-utility.md) **and deploy version 5.5.**
{% endhint %}

## Prerequisites

* Download the latest Cinchy Artifacts from the **Cinchy Releases Table > Kubernetes Artifacts** column _(Image 1)._ **For this upgrade, please download the Cinchy v5.6 k8s-template.zip file.**
* Review the [**template changes** ](v5.6-kubernetes.md#template-changes-kubernetes-5.6)for this upgrade.

## Configuring to the newest version

1. Navigate to your **cinchy.argocd** repository. **Delete** all existing folder structure except for the **.git folder/directory** and any custom changes you may have implemented.
2. Navigate to your **cinchy.kubernetes** repository. **Delete** all existing folder structure except for the **.git file.**

{% hint style="danger" %}
If you have **cinchy.kubernetes\cluster\_components\servicemesh\istio\istio-injection\argocd-ns.yaml file** and it's _not commented_ then please keep it as is. Changing this will delete your ArgoCD namespace, which will force you to delete everything from Kubernetes and redeploy.
{% endhint %}

4. Navigate to your **cinchy.terraform** repository. **Delete** all existing folder structure except for the **.git file.**
5. Navigate to your **cinchy.devops.automation** repository. **Delete** all existing folder structure except for the **.git file** and your **deployment.json.**
6. Open the new **Cinchy v5.6 k8s-template.zip** file you downloaded from the Cinchy Releases table and **check the files** into their respective cinchy.kubernete, cinchy.argocd, cinchy.terraform and cinchy.devops.automation repositories.
7. Navigate to the new **aws.json/azure.json files** and compare them with your current **deployment.json file**. Any additional fields in the new aws.json/azure.json files should be added to your current deployment.json.

{% hint style="warning" %}
Note that you may have changed the name of the **deployment.json file** during your original platform deployment. If so, ensure that you swap up the name wherever it appears in this document.
{% endhint %}

{% hint style="info" %}
Starting in Cinchy v5.4, you will have the option between Alpine or Debian based image tags for the listener, worker, and connections. **Using Debian tags will allow a Kubernetes deployment to be able to connect to a DB2 data source, and that option should be selected if you plan on leveraging a DB2 data sync.**

* When either installing or upgrading your platform, you can use the following Docker image tags for the **listener, worker, and connections:**
  * **"5.x.x" - Alpine**
  * **"5.x.x-debian" - Debian**
{% endhint %}

{% hint style="info" %}
**Perform this step only If you are upgrading to 5.6 on an SQL Server Database and didn't already make this change in any previous updates.** \\

Navigate to your **cinchy\_instance\_configs section > database\_connection\_string**, and add in the following value to the end of your string: [**TrustServerCertificate=True**](https://learn.microsoft.com/en-us/dotnet/api/system.data.sqlclient.sqlconnectionstringbuilder.trustservercertificate?view=dotnet-plat-ext-6.0)

<pre><code><strong>"cinchy_instance_configs": {
</strong>          "database_connection_string": "User ID=cinchy;Password=&#x3C;password>;Host=&#x3C;db_hostname>;Port=5432;Database=development;Timeout=300;Keepalive=300;TrustServerCertificate=True"},
</code></pre>
{% endhint %}

8. Open a shell/terminal from the cinchy.devops.automations directory and execute the following command:

```
dotnet Cinchy.DevOps.Automations.dll "deployment.json"
```

9. Commit all of your changes (if there were any) in each repository.
10. If there were any changes in your cinchy.argocd repository you may need to redeploy ArgoCD.
    1. Launch a shell/terminal with the working directory set to the root of the cinchy.argocd repository.
    2. Execute the following command to deploy ArgoCD:

```
bash deploy_argocd.sh
```

11. If there were any change to the cluster components, execute the following command from the cinchy.argocd repository:

```
bash deploy_cluster_components.sh
```

12. If there were any change to the Cinchy instance, execute the following command from the cinchy.argocd repository:

```
bash deploy_cinchy_components.sh
```

13. Log in to your ArgoCD application console and refresh the apps to ensure that all changes were picked up.

## Appendix A

#### Template changes (Kubernetes 5.6)

* The AWS EKS version has been [upgraded to support up to v1.24.](upgrading-aws-eks-kubernetes-version.md)
* We've added support for AWS EKS EBS volume encryption. By default EKS worker nodes will have gp3 storage class.
  * For current Cinchy environments you must keep your `eks_persistent_apps_storage_class` to **gp2** in your DevOps automation aws.json file.
  * <mark style="color:red;">If you want to move to</mark> <mark style="color:red;">**gp3 storage**</mark> <mark style="color:red;">or</mark> <mark style="color:red;">**gp3 storage and volume encryption,**</mark> <mark style="color:red;">you will have to delete any existing volumes/pvc's for Kafka, Redis, OpenSearch, Logging Operator and Event Listener with</mark> <mark style="color:red;">`statefulset`</mark><mark style="color:red;">. This ensures that ArgoCD will take care of recreation of resources.</mark>
  * If your Kafka cluster pods aren't coming back you must restart your Kafka operators.
  * You can verify the change by running the following command: _"kubectl get pvc --all-namespaces"._
* The Connections app has changed from StatefulSet to **Deployment**. The persistence volume has changed to `emptyDir`.
* We've modified the replica count from 1 to 2 for **istiod and istio ingress.**
* <mark style="color:red;">We've disabled the ArgoCD namespace:</mark> <mark style="color:red;">**istio injection.**</mark>
  * <mark style="color:red;">If this is already enabled on your environment you may keep it as is, such as keeping the</mark> <mark style="color:red;">**cinchy.kubernetes/cluster\_components/servicemesh/istio/istio-injection/argocd-ns.yaml file**</mark> <mark style="color:red;">as it's without commenting content in it.</mark>
* The **Istio namespace injection** has been removed.
  * <mark style="color:red;">If this is already enabled on your environment please keep it as is -- otherwise it will force you to redeploy all of your Kubernetes application components.</mark>
* We've upgraded the **AWS Secret Manager CSI Driver** to the latest version due to crashing pods.
* We've added support for the **EKS EBS CSI driver** in lieu of using in-tree EBS storage plugin.
* We've changed the **EKS Metrics server port number** in order to support newer versions of Kubernetes.
* We've set **fixed AWS Terraform providers** version for all components.
* We've installed the cluster autoscaler from **local charts** instead of remote charts.
* <mark style="color:red;">The deprecated</mark> <mark style="color:red;">`azurerm_sql_server`</mark> <mark style="color:red;">Terraform resource has been changed to</mark> <mark style="color:red;">**azurerm\_mssql\_server**</mark>
* <mark style="color:red;">The deprecated</mark> <mark style="color:red;">`azurerm_sql_database`</mark> <mark style="color:red;">resource has been changed to</mark> <mark style="color:red;">**azurerm\_mssql\_database**</mark>
* <mark style="color:red;">The deprecated azurerm\_sql\_failover\_group has been changed to</mark> <mark style="color:red;">**azurerm\_mssql\_failover\_group**</mark>
* <mark style="color:red;">The deprecated azurerm\_sql\_firewall\_rule has been changed to</mark> <mark style="color:red;">**azurerm\_mssql\_firewall\_rule**</mark>
